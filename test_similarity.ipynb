{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e21216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L185_0_A9_1 peaks: 195\n",
      "L185_0_A10_1 peaks: 202\n",
      "L186_0_A11_1 peaks: 213\n",
      "L186_0_A12_1 peaks: 219\n",
      "L187_0_B1_1 peaks: 211\n",
      "L187_0_B2_1 peaks: 215\n",
      "L291_0_A1_1 peaks: 215\n",
      "L291_0_A2_1 peaks: 201\n",
      "L100_0_G7_1 peaks: 207\n",
      "L100_0_G8_1 peaks: 224\n",
      "L101_0_A1_1 peaks: 229\n",
      "L101_0_A2_1 peaks: 200\n",
      "L103_0_A5_1 peaks: 195\n",
      "L103_0_A6_1 peaks: 197\n",
      "L125_0_E1_1 peaks: 209\n",
      "L125_0_E2_1 peaks: 192\n",
      "L126_0_E3_1 peaks: 197\n",
      "L126_0_E4_1 peaks: 198\n",
      "L128_0_E7_1 peaks: 204\n",
      "L128_0_E8_1 peaks: 193\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from BaselineRemoval import BaselineRemoval\n",
    "from scipy import signal\n",
    "import pickle\n",
    "import similarity as sim\n",
    "\n",
    "def find_peaks(y, cwt=False, smooth=False, baseline=False):\n",
    "    window, deg = 15, 2     \n",
    "    z = y\n",
    "    if smooth:\n",
    "        z = signal.savgol_filter(y, window, deg, deriv=0)\n",
    "\n",
    "    if baseline:\n",
    "        brm = BaselineRemoval(z)\n",
    "        z = brm.ZhangFit(lambda_=400,repitition=15, porder=1)\n",
    "    if cwt == True:\n",
    "        peaks = signal.find_peaks_cwt(z, [20])  \n",
    "    else:\n",
    "        dist, prom = 100, 600\n",
    "        peaks,_ = signal.find_peaks(z, distance=dist, prominence=prom)\n",
    "    return peaks, z\n",
    "\n",
    "def similar_to(pk1, pk2, method='jaccard', rank=5):\n",
    "    if method == 'rank':\n",
    "        return sim.rank_similarity(pk1, pk2, good_with=rank)\n",
    "    if method == 'weighted':\n",
    "        return sim.rank_similarity(pk1, pk2, good_with=rank, weighted=True)\n",
    "    else:\n",
    "        return sim.jaccard_similarity(pk1, pk2)\n",
    "    \n",
    "\n",
    "DATA_DIR_PATH = \"datasets/export\"\n",
    "PEAK_DIR_PATH = \"peaks\"\n",
    "files =[\n",
    "    \"L185_0_A9_1\", \"L185_0_A10_1\",\n",
    "    \"L186_0_A11_1\", \"L186_0_A12_1\",\n",
    "    \"L187_0_B1_1\", \"L187_0_B2_1\",\n",
    "    \"L291_0_A1_1\", \"L291_0_A2_1\",\n",
    "    \"L100_0_G7_1\", \"L100_0_G8_1\",\n",
    "    \"L101_0_A1_1\", \"L101_0_A2_1\",\n",
    "    \"L103_0_A5_1\", \"L103_0_A6_1\",\n",
    "    \"L125_0_E1_1\", \"L125_0_E2_1\",\n",
    "    \"L126_0_E3_1\", \"L126_0_E4_1\",\n",
    "    \"L128_0_E7_1\", \"L128_0_E8_1\",   \n",
    "]\n",
    "peaks_extracted = []\n",
    "n = len(files)\n",
    "for i in range(n):\n",
    "    df = None\n",
    "    df = pd.read_table(f\"{DATA_DIR_PATH}/{files[i]}.txt\",sep=\" \", header=None,names=['m/z', 'intensity']) \n",
    "    x, y = df['m/z'].to_numpy(), df['intensity'].to_numpy()\n",
    "\n",
    "    #  Peak detection in new datasets (time-consuming) \n",
    "\n",
    "    #     %time peaks, _ =  find_peaks(y, cwt=True, smooth=False, baseline=False)\n",
    "    #     print(f'{files[i]} peaks: {len(peaks)}')\n",
    "    #     with  open(f'{PEAK_DIR_PATH}/{files[i]}_peaks.pkl', 'wb') as peak_file\n",
    "    #         pickle.dump(peaks, peak_file)\n",
    "    #         peaks_extracted += [(x[peaks], y[peaks])]\n",
    "\n",
    "    #  Read peaks from pickle files\n",
    "\n",
    "    with open(f'{PEAK_DIR_PATH}/{files[i]}_peaks.pkl', 'rb') as peak_file:\n",
    "        peaks = pickle.load(peak_file)\n",
    "        print(f'{files[i]} peaks: {len(peaks)}')\n",
    "        peaks_extracted +=  [(x[peaks], y[peaks]) ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a9bfee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "n = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0ee4a05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank_Similarity:\n",
      "# 1*:\tL128_0_E7_1 & L128_0_E8_1\t0.1397*\n",
      "# 6*:\tL100_0_G7_1 & L100_0_G8_1\t0.0810*\n",
      "# 7*:\tL185_0_A9_1 & L185_0_A10_1\t0.0791*\n",
      "#11*:\tL186_0_A11_1 & L186_0_A12_1\t0.0696*\n",
      "#26*:\tL125_0_E1_1 & L125_0_E2_1\t0.0588*\n",
      "#29*:\tL291_0_A1_1 & L291_0_A2_1\t0.0581*\n",
      "#36*:\tL187_0_B1_1 & L187_0_B2_1\t0.0538*\n",
      "#54*:\tL101_0_A1_1 & L101_0_A2_1\t0.0467*\n",
      "#55*:\tL103_0_A5_1 & L103_0_A6_1\t0.0464*\n",
      "#64*:\tL126_0_E3_1 & L126_0_E4_1\t0.0440*\n",
      "algo_score=5.07 (closer to 1.0 is better )\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sim)\n",
    "rs = []\n",
    "print('Rank_Similarity:')\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        score = similar_to(peaks_extracted[i], peaks_extracted[j], method='rank', rank=2)\n",
    "        rs += [(i, j, score)]\n",
    "\n",
    "rs = sorted(rs, key=lambda x: x[2], reverse=True)\n",
    "algo_score=0\n",
    "for k, v in enumerate(rs):\n",
    "    i, j, s = v\n",
    "    tag = '*' if files[i][:4]==files[j][:4] else '' # tag the same strain id\n",
    "    if tag=='*':\n",
    "        print(f'#{k+1:2d}{tag}:\\t{files[i]} & {files[j]}\\t{s:.4f}{tag}')\n",
    "        algo_score += k;\n",
    "\n",
    "p = n/2      # pairs of files   \n",
    "print(f'algo_score={2*algo_score/float(p*(p+1)):.2f} (closer to 1.0 is better )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a4cd337",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted_Similarity:\n",
      "# 1*:\tL128_0_E7_1 & L128_0_E8_1\t0.0970*\n",
      "# 6*:\tL185_0_A9_1 & L185_0_A10_1\t0.0602*\n",
      "# 8*:\tL100_0_G7_1 & L100_0_G8_1\t0.0576*\n",
      "#18*:\tL125_0_E1_1 & L125_0_E2_1\t0.0499*\n",
      "#22*:\tL291_0_A1_1 & L291_0_A2_1\t0.0480*\n",
      "#23*:\tL186_0_A11_1 & L186_0_A12_1\t0.0470*\n",
      "#27*:\tL187_0_B1_1 & L187_0_B2_1\t0.0459*\n",
      "#37*:\tL101_0_A1_1 & L101_0_A2_1\t0.0439*\n",
      "#50*:\tL126_0_E3_1 & L126_0_E4_1\t0.0413*\n",
      "#62*:\tL103_0_A5_1 & L103_0_A6_1\t0.0364*\n",
      "algo_score=4.44 (closer to 1.0 is better )\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sim)\n",
    "rs = []\n",
    "print('Weighted_Similarity:')\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        score = similar_to(peaks_extracted[i], peaks_extracted[j], method='weighted', rank=2)\n",
    "        rs += [(i, j, score)]\n",
    "\n",
    "rs = sorted(rs, key=lambda x: x[2], reverse=True)\n",
    "algo_score=0\n",
    "for k, v in enumerate(rs):\n",
    "    i, j, s = v\n",
    "    tag = '*' if files[i][:4]==files[j][:4] else '' # tag the same strain id\n",
    "    if tag=='*':\n",
    "        print(f'#{k+1:2d}{tag}:\\t{files[i]} & {files[j]}\\t{s:.4f}{tag}')\n",
    "        algo_score += k;      \n",
    "\n",
    "p = n/2      # pairs of files         \n",
    "print(f'algo_score={2*algo_score/float(p*(p+1)):.2f} (closer to 1.0 is better )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e700e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c632d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
