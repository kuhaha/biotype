{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308fb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from BaselineRemoval import BaselineRemoval\n",
    "from scipy import signal\n",
    "import pickle\n",
    "import similarity as sim\n",
    "\n",
    "def find_peaks(y, cwt=False, smooth=False, baseline=False):\n",
    "    window, deg = 15, 2     \n",
    "    z = y\n",
    "    if smooth:\n",
    "        z = signal.savgol_filter(y, window, deg, deriv=0)\n",
    "\n",
    "    if baseline:\n",
    "        brm = BaselineRemoval(z)\n",
    "        z = brm.ZhangFit(lambda_=400,repitition=15, porder=1)\n",
    "    if cwt == True:\n",
    "        peaks = signal.find_peaks_cwt(z, [20])  \n",
    "    else:\n",
    "        dist, prom = 100, 600\n",
    "        peaks,_ = signal.find_peaks(z, distance=dist, prominence=prom)\n",
    "    return peaks, z\n",
    "\n",
    "def similar_to(pk1, pk2, method='jaccard', rank=5):\n",
    "    if method == 'rank':\n",
    "        return sim.rank_similarity(pk1, pk2, good_with=rank)\n",
    "    if method == 'weighted':\n",
    "        return sim.rank_similarity(pk1, pk2, good_with=rank, weighted=True)\n",
    "    else:\n",
    "        return sim.jaccard_similarity(pk1, pk2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d92aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PATH = \"datasets/export\"\n",
    "META_DIR_PATH = \"doc\"\n",
    "PEAK_DIR_PATH = \"peaks\"\n",
    "files =[\n",
    "    \"L185_0_A9_1\", \"L185_0_A10_1\",\n",
    "    \"L186_0_A11_1\", \"L186_0_A12_1\",\n",
    "    \"L187_0_B1_1\", \"L187_0_B2_1\",\n",
    "    \"L291_0_A1_1\", \"L291_0_A2_1\",\n",
    "    \"L100_0_G7_1\", \"L100_0_G8_1\",\n",
    "    \"L101_0_A1_1\", \"L101_0_A2_1\",\n",
    "    \"L103_0_A5_1\", \"L103_0_A6_1\",\n",
    "    \"L125_0_E1_1\", \"L125_0_E2_1\",\n",
    "    \"L126_0_E3_1\", \"L126_0_E4_1\",\n",
    "    \"L128_0_E7_1\", \"L128_0_E8_1\",   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45719cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>菌株名</th>\n",
       "      <th>血清型</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MALDITOFMS\n",
       "Listeria serial No.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L001</th>\n",
       "      <td>LM1</td>\n",
       "      <td>1/2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L002</th>\n",
       "      <td>LM3</td>\n",
       "      <td>1/2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L003</th>\n",
       "      <td>LM4</td>\n",
       "      <td>4b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L004</th>\n",
       "      <td>LM7</td>\n",
       "      <td>1/2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L005</th>\n",
       "      <td>LM8</td>\n",
       "      <td>1/2a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 菌株名   血清型\n",
       "MALDITOFMS\\nListeria serial No.           \n",
       "L001                             LM1  1/2a\n",
       "L002                             LM3  1/2a\n",
       "L003                             LM4    4b\n",
       "L004                             LM7  1/2a\n",
       "L005                             LM8  1/2a"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.read_csv(META_DIR_PATH + '/meta.csv', index_col=0,encoding='utf-8')\n",
    "meta[['菌株名','血清型']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd58c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "serotype = meta['血清型'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e21216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L185_0_A9_1 (NA) peaks: 197\n",
      "L185_0_A10_1 (NA) peaks: 199\n",
      "L186_0_A11_1 (NA) peaks: 208\n",
      "L186_0_A12_1 (NA) peaks: 220\n",
      "L187_0_B1_1 (NA) peaks: 205\n",
      "L187_0_B2_1 (NA) peaks: 220\n",
      "L291_0_A1_1 (NA) peaks: 221\n",
      "L291_0_A2_1 (NA) peaks: 200\n",
      "L100_0_G7_1 (1/2b) peaks: 208\n",
      "L100_0_G8_1 (1/2b) peaks: 226\n",
      "L101_0_A1_1 (1/2a) peaks: 226\n",
      "L101_0_A2_1 (1/2a) peaks: 195\n",
      "L103_0_A5_1 (1/2b) peaks: 192\n",
      "L103_0_A6_1 (1/2b) peaks: 188\n",
      "L125_0_E1_1 (1/2a) peaks: 211\n",
      "L125_0_E2_1 (1/2a) peaks: 192\n",
      "L126_0_E3_1 (1/2b) peaks: 191\n",
      "L126_0_E4_1 (1/2b) peaks: 194\n",
      "L128_0_E7_1 (1/2a) peaks: 198\n",
      "L128_0_E8_1 (1/2a) peaks: 189\n"
     ]
    }
   ],
   "source": [
    "peaks_extracted = []\n",
    "n = len(files)\n",
    "for i in range(n):\n",
    "    df = None\n",
    "    df = pd.read_table(f\"{DATA_DIR_PATH}/{files[i]}.txt\",sep=\" \", header=None,names=['m/z', 'intensity']) \n",
    "    x, y = df['m/z'].to_numpy(), df['intensity'].to_numpy()\n",
    "\n",
    "    #  Peak detection in new datasets (time-consuming) \n",
    "\n",
    "#     %time peaks, _ =  find_peaks(y, cwt=True, smooth=True, baseline=True)\n",
    "#     print(f'{files[i]} peaks: {len(peaks)}')\n",
    "#     with  open(f'{PEAK_DIR_PATH}/{files[i]}_peaks_ext.pkl', 'wb') as peak_file:\n",
    "#         pickle.dump(peaks, peak_file)\n",
    "#         peaks_extracted += [(x[peaks], y[peaks])]\n",
    "\n",
    "   #  Read peaks from pickle files\n",
    "\n",
    "    with open(f'{PEAK_DIR_PATH}/{files[i]}_peaks_ext.pkl', 'rb') as peak_file:\n",
    "        peaks = pickle.load(peak_file)\n",
    "        st = serotype.get(files[i][:4],'NA')\n",
    "        print(f'{files[i]} ({st}) peaks: {len(peaks)}')\n",
    "        peaks_extracted +=  [(x[peaks], y[peaks]) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a9bfee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "n = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ee4a05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank_Similarity:\n",
      "# 3*:\tL128_0_E7_1[1/2a] & L128_0_E8_1[1/2a]\t0.1015*\n",
      "#13*:\tL185_0_A9_1[NA] & L185_0_A10_1[NA]\t0.0753*\n",
      "#14*:\tL291_0_A1_1[NA] & L291_0_A2_1[NA]\t0.0723*\n",
      "#15*:\tL100_0_G7_1[1/2b] & L100_0_G8_1[1/2b]\t0.0708*\n",
      "#21*:\tL126_0_E3_1[1/2b] & L126_0_E4_1[1/2b]\t0.0599*\n",
      "#31*:\tL125_0_E1_1[1/2a] & L125_0_E2_1[1/2a]\t0.0550*\n",
      "#32*:\tL186_0_A11_1[NA] & L186_0_A12_1[NA]\t0.0541*\n",
      "#43*:\tL103_0_A5_1[1/2b] & L103_0_A6_1[1/2b]\t0.0481*\n",
      "#54*:\tL187_0_B1_1[NA] & L187_0_B2_1[NA]\t0.0440*\n",
      "#56*:\tL101_0_A1_1[1/2a] & L101_0_A2_1[1/2a]\t0.0438*\n",
      "algo_score=4.95 (closer to 1.0 is better )\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sim)\n",
    "rs = []\n",
    "print('Rank_Similarity:')\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        score = similar_to(peaks_extracted[i], peaks_extracted[j], method='rank', rank=2)\n",
    "        rs += [(i, j, score)]\n",
    "\n",
    "rs = sorted(rs, key=lambda x: x[2], reverse=True)\n",
    "algo_score=0\n",
    "for k, v in enumerate(rs):\n",
    "    i, j, s = v\n",
    "    tag = '*' if files[i][:4]==files[j][:4] else '' # tag the same strain id\n",
    "    st1 =  serotype.get(files[i][:4],'NA')\n",
    "    st2 =  serotype.get(files[j][:4],'NA')\n",
    "    if tag=='*':\n",
    "        algo_score += k\n",
    "        print(f'#{k+1:2d}{tag}:\\t{files[i]}[{st1}] & {files[j]}[{st2}]\\t{s:.4f}{tag}')\n",
    "\n",
    "p = n/2      # pairs of files   \n",
    "print(f'algo_score={2*algo_score/float(p*(p+1)):.2f} (closer to 1.0 is better )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a4cd337",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted_Similarity:\n",
      "# 1*:\tL128_0_E7_1 & L128_0_E8_1\t0.0372*\n",
      "# 9*:\tL185_0_A9_1 & L185_0_A10_1\t0.0280*\n",
      "#12*:\tL126_0_E3_1 & L126_0_E4_1\t0.0268*\n",
      "#13*:\tL291_0_A1_1 & L291_0_A2_1\t0.0264*\n",
      "#14*:\tL125_0_E1_1 & L125_0_E2_1\t0.0258*\n",
      "#15*:\tL101_0_A1_1 & L101_0_A2_1\t0.0258*\n",
      "#25*:\tL100_0_G7_1 & L100_0_G8_1\t0.0238*\n",
      "#31*:\tL187_0_B1_1 & L187_0_B2_1\t0.0231*\n",
      "#75*:\tL103_0_A5_1 & L103_0_A6_1\t0.0185*\n",
      "#86*:\tL186_0_A11_1 & L186_0_A12_1\t0.0179*\n",
      "algo_score=4.93 (closer to 1.0 is better )\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sim)\n",
    "rs = []\n",
    "print('Weighted_Similarity:')\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        score = similar_to(peaks_extracted[i], peaks_extracted[j], method='weighted', rank=2)\n",
    "        rs += [(i, j, score)]\n",
    "\n",
    "rs = sorted(rs, key=lambda x: x[2], reverse=True)\n",
    "algo_score=0\n",
    "for k, v in enumerate(rs):\n",
    "    i, j, s = v\n",
    "    tag = '*' if files[i][:4]==files[j][:4] else '' # tag the same strain id\n",
    "    if tag=='*':\n",
    "        print(f'#{k+1:2d}{tag}:\\t{files[i]} & {files[j]}\\t{s:.4f}{tag}')\n",
    "        algo_score += k;      \n",
    "\n",
    "p = n/2      # pairs of files         \n",
    "print(f'algo_score={2*algo_score/float(p*(p+1)):.2f} (closer to 1.0 is better )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c632d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
