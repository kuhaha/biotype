{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308fb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import biotype.similarity as sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8d92aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L100_0_G7_1.txt\n",
      "L100_0_G8_1.txt\n",
      "L101_0_A1_1.txt\n",
      "L101_0_A2_1.txt\n",
      "L102_0_A3_1.txt\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR_PATH = \"datasets/export\"\n",
    "META_DIR_PATH = \"doc\"\n",
    "PEAK_DIR_PATH = \"extracted_peaks\"\n",
    "files =  os.listdir(DATA_DIR_PATH)\n",
    "files = [fn  for fn in files if fn.endswith(\".txt\")]\n",
    "files = sorted(files)\n",
    "for file in files[:5] :\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45719cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>菌株名</th>\n",
       "      <th>血清型</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MALDITOFMS\n",
       "Listeria serial No.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L001</th>\n",
       "      <td>LM1</td>\n",
       "      <td>1/2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L002</th>\n",
       "      <td>LM3</td>\n",
       "      <td>1/2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L003</th>\n",
       "      <td>LM4</td>\n",
       "      <td>4b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L004</th>\n",
       "      <td>LM7</td>\n",
       "      <td>1/2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L005</th>\n",
       "      <td>LM8</td>\n",
       "      <td>1/2a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 菌株名   血清型\n",
       "MALDITOFMS\\nListeria serial No.           \n",
       "L001                             LM1  1/2a\n",
       "L002                             LM3  1/2a\n",
       "L003                             LM4    4b\n",
       "L004                             LM7  1/2a\n",
       "L005                             LM8  1/2a"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.read_csv(META_DIR_PATH + '/meta.csv', index_col=0,encoding='utf-8')\n",
    "meta[['菌株名','血清型']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd58c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "serotype = meta['血清型'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e21216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L100_0_G7_1 (1/2b) peaks: 208\n",
      "L100_0_G8_1 (1/2b) peaks: 226\n",
      "L101_0_A1_1 (1/2a) peaks: 226\n",
      "L101_0_A2_1 (1/2a) peaks: 195\n",
      "L102_0_A3_1 (1/2b) peaks: 208\n",
      "L102_0_A4_1 (1/2b) peaks: 200\n",
      "L103_0_A5_1 (1/2b) peaks: 192\n",
      "L103_0_A6_1 (1/2b) peaks: 188\n",
      "L104_0_A7_1 (1/2a) peaks: 193\n",
      "L104_0_A8_1 (1/2a) peaks: 221\n",
      "L105_0_A10_1 (1/2b) peaks: 186\n",
      "L105_0_A9_1 (1/2b) peaks: 204\n",
      "L106_0_A11_1 (1/2a) peaks: 208\n",
      "L106_0_A12_1 (1/2a) peaks: 196\n",
      "L107_0_B1_1 (1/2b) peaks: 190\n",
      "L107_0_B2_1 (1/2b) peaks: 195\n",
      "L108_0_B3_1 (UT) peaks: 213\n",
      "L108_0_B4_1 (UT) peaks: 186\n",
      "L109_0_B5_1 (UT) peaks: 214\n",
      "L109_0_B6_1 (UT) peaks: 205\n",
      "L10_0_G10_1 (NA) peaks: 189\n",
      "L10_0_G9_1 (NA) peaks: 197\n",
      "L110_0_B7_1 (UT) peaks: 190\n",
      "L110_0_B8_1 (UT) peaks: 203\n",
      "L111_0_B10_1 (UT) peaks: 240\n",
      "L111_0_B9_1 (UT) peaks: 214\n",
      "L112_0_B11_1 (1/2a) peaks: 226\n",
      "L112_0_B12_1 (1/2a) peaks: 223\n",
      "L113_0_C1_1 (UT) peaks: 202\n",
      "L113_0_C2_1 (UT) peaks: 210\n",
      "L114_0_C3_1 (UT) peaks: 289\n",
      "L114_0_C4_1 (UT) peaks: 265\n",
      "L115_0_C5_1 (3a) peaks: 207\n",
      "L115_0_C6_1 (3a) peaks: 195\n",
      "L116_0_C7_1 (nan) peaks: 201\n",
      "L116_0_C8_1 (nan) peaks: 201\n",
      "L117_0_C10_1 (1/2a) peaks: 233\n",
      "L117_0_C9_1 (1/2a) peaks: 206\n",
      "L118_0_C11_1 (1/2a) peaks: 189\n",
      "L118_0_C12_1 (1/2a) peaks: 215\n",
      "L119_0_D1_1 (1/2a) peaks: 202\n",
      "L119_0_D2_1 (1/2a) peaks: 198\n",
      "L11_0_G11_1 (NA) peaks: 203\n",
      "L11_0_G12_1 (NA) peaks: 225\n",
      "L120_0_D3_1 (1/2a) peaks: 191\n",
      "L120_0_D4_1 (1/2a) peaks: 197\n",
      "L121_0_D5_1 (1/2b) peaks: 193\n",
      "L121_0_D6_1 (1/2b) peaks: 180\n",
      "L122_0_D7_1 (1/2b) peaks: 211\n",
      "L122_0_D8_1 (1/2b) peaks: 200\n",
      "L123_0_D10_1 (1/2a) peaks: 194\n",
      "L123_0_D9_1 (1/2a) peaks: 205\n",
      "L124_0_D11_1 (1/2a) peaks: 226\n",
      "L124_0_D12_1 (1/2a) peaks: 186\n",
      "L125_0_E1_1 (1/2a) peaks: 211\n",
      "L125_0_E2_1 (1/2a) peaks: 192\n",
      "L126_0_E3_1 (1/2b) peaks: 191\n",
      "L126_0_E4_1 (1/2b) peaks: 194\n",
      "L127_0_E5_1 (UT) peaks: 209\n",
      "L127_0_E6_1 (UT) peaks: 186\n",
      "L128_0_E7_1 (1/2a) peaks: 198\n",
      "L128_0_E8_1 (1/2a) peaks: 189\n",
      "L129_0_E10_1 (1/2b) peaks: 185\n",
      "L129_0_E9_1 (1/2b) peaks: 186\n",
      "L12_0_H1_1 (NA) peaks: 188\n",
      "L12_0_H2_1 (NA) peaks: 182\n",
      "L130_0_E11_1 (1/2a) peaks: 232\n",
      "L130_0_E12_1 (1/2a) peaks: 195\n",
      "L131_0_F1_1 (1/2a) peaks: 201\n",
      "L131_0_F2_1 (1/2a) peaks: 218\n",
      "L132_0_F3_1 (4b) peaks: 206\n",
      "L132_0_F4_1 (4b) peaks: 187\n",
      "L133_0_F5_1 (4b) peaks: 201\n",
      "L133_0_F6_1 (4b) peaks: 191\n",
      "L134_0_F7_1 (4b) peaks: 240\n",
      "L134_0_F8_1 (4b) peaks: 214\n",
      "L135_0_F10_1 (4b) peaks: 218\n",
      "L135_0_F9_1 (4b) peaks: 205\n",
      "L136_0_F11_1 (4b) peaks: 230\n",
      "L136_0_F12_1 (4b) peaks: 212\n",
      "L137_0_G1_1 (4b) peaks: 217\n",
      "L137_0_G2_1 (4b) peaks: 194\n",
      "L138_0_G3_1 (1/2c) peaks: 194\n",
      "L138_0_G4_1 (1/2c) peaks: 196\n",
      "L139_0_G5_1 (1/2c) peaks: 201\n",
      "L139_0_G6_1 (1/2c) peaks: 200\n",
      "L13_0_H3_1 (NA) peaks: 193\n",
      "L13_0_H4_1 (NA) peaks: 196\n",
      "L140_0_G7_1 (1/2c) peaks: 218\n",
      "L140_0_G8_1 (1/2c) peaks: 203\n",
      "L14_0_H5_1 (NA) peaks: 216\n",
      "L14_0_H6_1 (NA) peaks: 218\n",
      "L185_0_A10_1 (NA) peaks: 199\n",
      "L185_0_A9_1 (NA) peaks: 197\n",
      "L186_0_A11_1 (NA) peaks: 208\n",
      "L186_0_A12_1 (NA) peaks: 220\n",
      "L187_0_B1_1 (NA) peaks: 205\n",
      "L187_0_B2_1 (NA) peaks: 220\n",
      "L1_0_F3_1 (NA) peaks: 204\n",
      "L1_0_F4_1 (NA) peaks: 182\n",
      "L291_0_A1_1 (NA) peaks: 221\n",
      "L291_0_A2_1 (NA) peaks: 200\n"
     ]
    }
   ],
   "source": [
    "peaks_extracted = []\n",
    "n = len(files)\n",
    "for i in range(n):\n",
    "    df = None\n",
    "    df = pd.read_table(f\"{DATA_DIR_PATH}/{files[i]}\",sep=\" \", header=None,names=['m/z', 'intensity']) \n",
    "    x, y = df['m/z'].to_numpy(), df['intensity'].to_numpy()\n",
    "\n",
    "    pickle_file = files[i][:-4] # trim '.txt'\n",
    "    with open(f'{PEAK_DIR_PATH}/{pickle_file}_peaks.pkl', 'rb') as peak_file:\n",
    "        peaks = pickle.load(peak_file)\n",
    "        st = serotype.get(files[i][:4],'NA')\n",
    "        print(f'{pickle_file} ({st}) peaks: {len(peaks)}')\n",
    "        peaks_extracted +=  [(x[peaks], y[peaks]) ] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a9bfee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "n = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard_Similarity:\n",
      "# 6*:\tL121_0_D5_1.txt[1/2b] & L121_0_D6_1.txt[1/2b]\t0.4802*\n",
      "# 7*:\tL129_0_E10_1.txt[1/2b] & L129_0_E9_1.txt[1/2b]\t0.4781*\n",
      "#12*:\tL128_0_E7_1.txt[1/2a] & L128_0_E8_1.txt[1/2a]\t0.4715*\n",
      "#16*:\tL12_0_H1_1.txt[NA] & L12_0_H2_1.txt[NA]\t0.4683*\n",
      "#21*:\tL116_0_C7_1.txt[nan] & L116_0_C8_1.txt[nan]\t0.4672*\n",
      "#23*:\tL138_0_G3_1.txt[1/2c] & L138_0_G4_1.txt[1/2c]\t0.4662*\n",
      "#43*:\tL126_0_E3_1.txt[1/2b] & L126_0_E4_1.txt[1/2b]\t0.4474*\n",
      "#45*:\tL122_0_D7_1.txt[1/2b] & L122_0_D8_1.txt[1/2b]\t0.4472*\n",
      "#49*:\tL123_0_D10_1.txt[1/2a] & L123_0_D9_1.txt[1/2a]\t0.4457*\n",
      "#51*:\tL119_0_D1_1.txt[1/2a] & L119_0_D2_1.txt[1/2a]\t0.4440*\n",
      "#60*:\tL139_0_G5_1.txt[1/2c] & L139_0_G6_1.txt[1/2c]\t0.4424*\n",
      "#65*:\tL13_0_H3_1.txt[NA] & L13_0_H4_1.txt[NA]\t0.4407*\n",
      "#69*:\tL110_0_B7_1.txt[UT] & L110_0_B8_1.txt[UT]\t0.4396*\n",
      "#80*:\tL107_0_B1_1.txt[1/2b] & L107_0_B2_1.txt[1/2b]\t0.4366*\n",
      "#89*:\tL132_0_F3_1.txt[4b] & L132_0_F4_1.txt[4b]\t0.4343*\n",
      "#108*:\tL101_0_A1_1.txt[1/2a] & L101_0_A2_1.txt[1/2a]\t0.4320*\n",
      "#110*:\tL102_0_A3_1.txt[1/2b] & L102_0_A4_1.txt[1/2b]\t0.4316*\n",
      "#129*:\tL105_0_A10_1.txt[1/2b] & L105_0_A9_1.txt[1/2b]\t0.4286*\n",
      "#162*:\tL135_0_F10_1.txt[4b] & L135_0_F9_1.txt[4b]\t0.4242*\n",
      "#189*:\tL120_0_D3_1.txt[1/2a] & L120_0_D4_1.txt[1/2a]\t0.4212*\n",
      "#193*:\tL115_0_C5_1.txt[3a] & L115_0_C6_1.txt[3a]\t0.4205*\n",
      "#205*:\tL185_0_A10_1.txt[NA] & L185_0_A9_1.txt[NA]\t0.4194*\n",
      "#213*:\tL103_0_A5_1.txt[1/2b] & L103_0_A6_1.txt[1/2b]\t0.4179*\n",
      "#245*:\tL133_0_F5_1.txt[4b] & L133_0_F6_1.txt[4b]\t0.4152*\n",
      "#348*:\tL113_0_C1_1.txt[UT] & L113_0_C2_1.txt[UT]\t0.4061*\n",
      "#538*:\tL118_0_C11_1.txt[1/2a] & L118_0_C12_1.txt[1/2a]\t0.3931*\n",
      "#580*:\tL130_0_E11_1.txt[1/2a] & L130_0_E12_1.txt[1/2a]\t0.3909*\n",
      "#593*:\tL108_0_B3_1.txt[UT] & L108_0_B4_1.txt[UT]\t0.3902*\n",
      "#711*:\tL1_0_F3_1.txt[NA] & L1_0_F4_1.txt[NA]\t0.3835*\n",
      "#880*:\tL125_0_E1_1.txt[1/2a] & L125_0_E2_1.txt[1/2a]\t0.3754*\n",
      "#907*:\tL106_0_A11_1.txt[1/2a] & L106_0_A12_1.txt[1/2a]\t0.3741*\n",
      "#925*:\tL10_0_G10_1.txt[NA] & L10_0_G9_1.txt[NA]\t0.3737*\n",
      "#929*:\tL124_0_D11_1.txt[1/2a] & L124_0_D12_1.txt[1/2a]\t0.3733*\n",
      "#1102*:\tL140_0_G7_1.txt[1/2c] & L140_0_G8_1.txt[1/2c]\t0.3669*\n",
      "#1249*:\tL104_0_A7_1.txt[1/2a] & L104_0_A8_1.txt[1/2a]\t0.3618*\n",
      "#1296*:\tL109_0_B5_1.txt[UT] & L109_0_B6_1.txt[UT]\t0.3604*\n",
      "#1297*:\tL131_0_F1_1.txt[1/2a] & L131_0_F2_1.txt[1/2a]\t0.3604*\n",
      "#1345*:\tL186_0_A11_1.txt[NA] & L186_0_A12_1.txt[NA]\t0.3587*\n",
      "#1413*:\tL112_0_B11_1.txt[1/2a] & L112_0_B12_1.txt[1/2a]\t0.3565*\n",
      "#1421*:\tL137_0_G1_1.txt[4b] & L137_0_G2_1.txt[4b]\t0.3564*\n",
      "#1561*:\tL14_0_H5_1.txt[NA] & L14_0_H6_1.txt[NA]\t0.3520*\n",
      "#1893*:\tL11_0_G11_1.txt[NA] & L11_0_G12_1.txt[NA]\t0.3417*\n",
      "#1968*:\tL127_0_E5_1.txt[UT] & L127_0_E6_1.txt[UT]\t0.3390*\n",
      "#2063*:\tL187_0_B1_1.txt[NA] & L187_0_B2_1.txt[NA]\t0.3365*\n",
      "#2255*:\tL111_0_B10_1.txt[UT] & L111_0_B9_1.txt[UT]\t0.3314*\n",
      "#2297*:\tL117_0_C10_1.txt[1/2a] & L117_0_C9_1.txt[1/2a]\t0.3303*\n",
      "#2405*:\tL134_0_F7_1.txt[4b] & L134_0_F8_1.txt[4b]\t0.3275*\n",
      "#2562*:\tL136_0_F11_1.txt[4b] & L136_0_F12_1.txt[4b]\t0.3234*\n",
      "#3131*:\tL100_0_G7_1.txt[1/2b] & L100_0_G8_1.txt[1/2b]\t0.3072*\n",
      "#3430*:\tL291_0_A1_1.txt[NA] & L291_0_A2_1.txt[NA]\t0.2994*\n",
      "#4834*:\tL114_0_C3_1.txt[UT] & L114_0_C4_1.txt[UT]\t0.2422*\n",
      "algo_score=34.75 (closer to 1.0 is better )\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sim)\n",
    "rs = []\n",
    "print('Jaccard_Similarity:')\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        score = sim.similar_to(peaks_extracted[i], peaks_extracted[j], method='jaccard', rank=2)\n",
    "        rs += [(i, j, score)]\n",
    "\n",
    "rs = sorted(rs, key=lambda x: x[2], reverse=True)\n",
    "algo_score=0\n",
    "for k, v in enumerate(rs):\n",
    "    i, j, s = v\n",
    "    tag = '*' if files[i][:4]==files[j][:4] else '' # tag the same strain id\n",
    "    st1 =  serotype.get(files[i][:4],'NA')\n",
    "    st2 =  serotype.get(files[j][:4],'NA')\n",
    "    if tag=='*':\n",
    "        algo_score += k\n",
    "        print(f'#{k+1:2d}{tag}:\\t{files[i]}[{st1}] & {files[j]}[{st2}]\\t{s:.4f}{tag}')\n",
    "\n",
    "p = n/2      # pairs of files   \n",
    "print(f'algo_score={2*algo_score/float(p*(p+1)):.2f} (closer to 1.0 is better )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ee4a05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank_Similarity:\n",
      "# 1*:\tL138_0_G3_1.txt[1/2c] & L138_0_G4_1.txt[1/2c]\t0.1917*\n",
      "# 2*:\tL115_0_C5_1.txt[3a] & L115_0_C6_1.txt[3a]\t0.1731*\n",
      "# 4*:\tL12_0_H1_1.txt[NA] & L12_0_H2_1.txt[NA]\t0.1548*\n",
      "# 7*:\tL116_0_C7_1.txt[nan] & L116_0_C8_1.txt[nan]\t0.1423*\n",
      "#10*:\tL122_0_D7_1.txt[1/2b] & L122_0_D8_1.txt[1/2b]\t0.1408*\n",
      "#13*:\tL121_0_D5_1.txt[1/2b] & L121_0_D6_1.txt[1/2b]\t0.1349*\n",
      "#19*:\tL129_0_E10_1.txt[1/2b] & L129_0_E9_1.txt[1/2b]\t0.1275*\n",
      "#28*:\tL120_0_D3_1.txt[1/2a] & L120_0_D4_1.txt[1/2a]\t0.1209*\n",
      "#32*:\tL119_0_D1_1.txt[1/2a] & L119_0_D2_1.txt[1/2a]\t0.1191*\n",
      "#43*:\tL13_0_H3_1.txt[NA] & L13_0_H4_1.txt[NA]\t0.1148*\n",
      "#61*:\tL102_0_A3_1.txt[1/2b] & L102_0_A4_1.txt[1/2b]\t0.1088*\n",
      "#65*:\tL128_0_E7_1.txt[1/2a] & L128_0_E8_1.txt[1/2a]\t0.1065*\n",
      "#99*:\tL113_0_C1_1.txt[UT] & L113_0_C2_1.txt[UT]\t0.0990*\n",
      "#102*:\tL136_0_F11_1.txt[4b] & L136_0_F12_1.txt[4b]\t0.0988*\n",
      "#108*:\tL123_0_D10_1.txt[1/2a] & L123_0_D9_1.txt[1/2a]\t0.0978*\n",
      "#109*:\tL131_0_F1_1.txt[1/2a] & L131_0_F2_1.txt[1/2a]\t0.0974*\n",
      "#177*:\tL118_0_C11_1.txt[1/2a] & L118_0_C12_1.txt[1/2a]\t0.0897*\n",
      "#196*:\tL11_0_G11_1.txt[NA] & L11_0_G12_1.txt[NA]\t0.0878*\n",
      "#211*:\tL133_0_F5_1.txt[4b] & L133_0_F6_1.txt[4b]\t0.0866*\n",
      "#236*:\tL110_0_B7_1.txt[UT] & L110_0_B8_1.txt[UT]\t0.0842*\n",
      "#286*:\tL105_0_A10_1.txt[1/2b] & L105_0_A9_1.txt[1/2b]\t0.0806*\n",
      "#311*:\tL139_0_G5_1.txt[1/2c] & L139_0_G6_1.txt[1/2c]\t0.0791*\n",
      "#326*:\tL106_0_A11_1.txt[1/2a] & L106_0_A12_1.txt[1/2a]\t0.0782*\n",
      "#396*:\tL185_0_A10_1.txt[NA] & L185_0_A9_1.txt[NA]\t0.0753*\n",
      "#397*:\tL1_0_F3_1.txt[NA] & L1_0_F4_1.txt[NA]\t0.0753*\n",
      "#474*:\tL14_0_H5_1.txt[NA] & L14_0_H6_1.txt[NA]\t0.0717*\n",
      "#493*:\tL107_0_B1_1.txt[1/2b] & L107_0_B2_1.txt[1/2b]\t0.0709*\n",
      "#549*:\tL100_0_G7_1.txt[1/2b] & L100_0_G8_1.txt[1/2b]\t0.0693*\n",
      "#759*:\tL186_0_A11_1.txt[NA] & L186_0_A12_1.txt[NA]\t0.0635*\n",
      "#770*:\tL124_0_D11_1.txt[1/2a] & L124_0_D12_1.txt[1/2a]\t0.0633*\n",
      "#887*:\tL127_0_E5_1.txt[UT] & L127_0_E6_1.txt[UT]\t0.0610*\n",
      "#962*:\tL137_0_G1_1.txt[4b] & L137_0_G2_1.txt[4b]\t0.0594*\n",
      "#1000*:\tL291_0_A1_1.txt[NA] & L291_0_A2_1.txt[NA]\t0.0586*\n",
      "#1012*:\tL132_0_F3_1.txt[4b] & L132_0_F4_1.txt[4b]\t0.0584*\n",
      "#1064*:\tL112_0_B11_1.txt[1/2a] & L112_0_B12_1.txt[1/2a]\t0.0574*\n",
      "#1353*:\tL126_0_E3_1.txt[1/2b] & L126_0_E4_1.txt[1/2b]\t0.0526*\n",
      "#1446*:\tL125_0_E1_1.txt[1/2a] & L125_0_E2_1.txt[1/2a]\t0.0512*\n",
      "#1451*:\tL101_0_A1_1.txt[1/2a] & L101_0_A2_1.txt[1/2a]\t0.0510*\n",
      "#1487*:\tL135_0_F10_1.txt[4b] & L135_0_F9_1.txt[4b]\t0.0505*\n",
      "#1629*:\tL103_0_A5_1.txt[1/2b] & L103_0_A6_1.txt[1/2b]\t0.0485*\n",
      "#1740*:\tL187_0_B1_1.txt[NA] & L187_0_B2_1.txt[NA]\t0.0472*\n",
      "#1764*:\tL111_0_B10_1.txt[UT] & L111_0_B9_1.txt[UT]\t0.0469*\n",
      "#2170*:\tL140_0_G7_1.txt[1/2c] & L140_0_G8_1.txt[1/2c]\t0.0422*\n",
      "#2411*:\tL104_0_A7_1.txt[1/2a] & L104_0_A8_1.txt[1/2a]\t0.0395*\n",
      "#2447*:\tL10_0_G10_1.txt[NA] & L10_0_G9_1.txt[NA]\t0.0391*\n",
      "#2475*:\tL109_0_B5_1.txt[UT] & L109_0_B6_1.txt[UT]\t0.0390*\n",
      "#2533*:\tL108_0_B3_1.txt[UT] & L108_0_B4_1.txt[UT]\t0.0383*\n",
      "#2562*:\tL134_0_F7_1.txt[4b] & L134_0_F8_1.txt[4b]\t0.0380*\n",
      "#3074*:\tL117_0_C10_1.txt[1/2a] & L117_0_C9_1.txt[1/2a]\t0.0333*\n",
      "#3492*:\tL130_0_E11_1.txt[1/2a] & L130_0_E12_1.txt[1/2a]\t0.0293*\n",
      "#4691*:\tL114_0_C3_1.txt[UT] & L114_0_C4_1.txt[UT]\t0.0157*\n",
      "algo_score=36.11 (closer to 1.0 is better )\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sim)\n",
    "rs = []\n",
    "print('Rank_Similarity:')\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        score = sim.similar_to(peaks_extracted[i], peaks_extracted[j], method='rank', rank=2)\n",
    "        rs += [(i, j, score)]\n",
    "\n",
    "rs = sorted(rs, key=lambda x: x[2], reverse=True)\n",
    "algo_score=0\n",
    "for k, v in enumerate(rs):\n",
    "    i, j, s = v\n",
    "    tag = '*' if files[i][:4]==files[j][:4] else '' # tag the same strain id\n",
    "    st1 =  serotype.get(files[i][:4],'NA')\n",
    "    st2 =  serotype.get(files[j][:4],'NA')\n",
    "    if tag=='*':\n",
    "        algo_score += k\n",
    "        print(f'#{k+1:2d}{tag}:\\t{files[i]}[{st1}] & {files[j]}[{st2}]\\t{s:.4f}{tag}')\n",
    "\n",
    "p = n/2      # pairs of files   \n",
    "print(f'algo_score={2*algo_score/float(p*(p+1)):.2f} (closer to 1.0 is better )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a4cd337",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted_Similarity:\n",
      "# 1*:\tL138_0_G3_1.txt & L138_0_G4_1.txt\t0.0486*\n",
      "# 2*:\tL115_0_C5_1.txt & L115_0_C6_1.txt\t0.0443*\n",
      "# 4*:\tL12_0_H1_1.txt & L12_0_H2_1.txt\t0.0424*\n",
      "# 8*:\tL121_0_D5_1.txt & L121_0_D6_1.txt\t0.0406*\n",
      "#10*:\tL129_0_E10_1.txt & L129_0_E9_1.txt\t0.0396*\n",
      "#18*:\tL128_0_E7_1.txt & L128_0_E8_1.txt\t0.0380*\n",
      "#20*:\tL116_0_C7_1.txt & L116_0_C8_1.txt\t0.0378*\n",
      "#22*:\tL13_0_H3_1.txt & L13_0_H4_1.txt\t0.0376*\n",
      "#27*:\tL119_0_D1_1.txt & L119_0_D2_1.txt\t0.0371*\n",
      "#28*:\tL122_0_D7_1.txt & L122_0_D8_1.txt\t0.0369*\n",
      "#49*:\tL102_0_A3_1.txt & L102_0_A4_1.txt\t0.0353*\n",
      "#56*:\tL120_0_D3_1.txt & L120_0_D4_1.txt\t0.0348*\n",
      "#71*:\tL123_0_D10_1.txt & L123_0_D9_1.txt\t0.0342*\n",
      "#92*:\tL118_0_C11_1.txt & L118_0_C12_1.txt\t0.0334*\n",
      "#108*:\tL113_0_C1_1.txt & L113_0_C2_1.txt\t0.0329*\n",
      "#161*:\tL110_0_B7_1.txt & L110_0_B8_1.txt\t0.0317*\n",
      "#163*:\tL131_0_F1_1.txt & L131_0_F2_1.txt\t0.0317*\n",
      "#171*:\tL136_0_F11_1.txt & L136_0_F12_1.txt\t0.0316*\n",
      "#181*:\tL139_0_G5_1.txt & L139_0_G6_1.txt\t0.0312*\n",
      "#223*:\tL105_0_A10_1.txt & L105_0_A9_1.txt\t0.0304*\n",
      "#236*:\tL107_0_B1_1.txt & L107_0_B2_1.txt\t0.0302*\n",
      "#298*:\tL106_0_A11_1.txt & L106_0_A12_1.txt\t0.0294*\n",
      "#332*:\tL133_0_F5_1.txt & L133_0_F6_1.txt\t0.0290*\n",
      "#442*:\tL185_0_A10_1.txt & L185_0_A9_1.txt\t0.0280*\n",
      "#495*:\tL11_0_G11_1.txt & L11_0_G12_1.txt\t0.0276*\n",
      "#571*:\tL1_0_F3_1.txt & L1_0_F4_1.txt\t0.0269*\n",
      "#589*:\tL101_0_A1_1.txt & L101_0_A2_1.txt\t0.0268*\n",
      "#729*:\tL135_0_F10_1.txt & L135_0_F9_1.txt\t0.0261*\n",
      "#739*:\tL126_0_E3_1.txt & L126_0_E4_1.txt\t0.0261*\n",
      "#752*:\tL125_0_E1_1.txt & L125_0_E2_1.txt\t0.0260*\n",
      "#906*:\tL291_0_A1_1.txt & L291_0_A2_1.txt\t0.0252*\n",
      "#954*:\tL132_0_F3_1.txt & L132_0_F4_1.txt\t0.0250*\n",
      "#1041*:\tL112_0_B11_1.txt & L112_0_B12_1.txt\t0.0246*\n",
      "#1053*:\tL137_0_G1_1.txt & L137_0_G2_1.txt\t0.0245*\n",
      "#1055*:\tL14_0_H5_1.txt & L14_0_H6_1.txt\t0.0245*\n",
      "#1151*:\tL103_0_A5_1.txt & L103_0_A6_1.txt\t0.0242*\n",
      "#1152*:\tL108_0_B3_1.txt & L108_0_B4_1.txt\t0.0241*\n",
      "#1300*:\tL186_0_A11_1.txt & L186_0_A12_1.txt\t0.0236*\n",
      "#1336*:\tL187_0_B1_1.txt & L187_0_B2_1.txt\t0.0235*\n",
      "#1400*:\tL100_0_G7_1.txt & L100_0_G8_1.txt\t0.0233*\n",
      "#1444*:\tL127_0_E5_1.txt & L127_0_E6_1.txt\t0.0231*\n",
      "#1462*:\tL124_0_D11_1.txt & L124_0_D12_1.txt\t0.0231*\n",
      "#1467*:\tL111_0_B10_1.txt & L111_0_B9_1.txt\t0.0230*\n",
      "#1782*:\tL109_0_B5_1.txt & L109_0_B6_1.txt\t0.0220*\n",
      "#2135*:\tL130_0_E11_1.txt & L130_0_E12_1.txt\t0.0207*\n",
      "#2780*:\tL134_0_F7_1.txt & L134_0_F8_1.txt\t0.0187*\n",
      "#3209*:\tL117_0_C10_1.txt & L117_0_C9_1.txt\t0.0174*\n",
      "#3391*:\tL140_0_G7_1.txt & L140_0_G8_1.txt\t0.0168*\n",
      "#3408*:\tL104_0_A7_1.txt & L104_0_A8_1.txt\t0.0168*\n",
      "#4275*:\tL10_0_G10_1.txt & L10_0_G9_1.txt\t0.0124*\n",
      "#4751*:\tL114_0_C3_1.txt & L114_0_C4_1.txt\t0.0087*\n",
      "algo_score=36.20 (closer to 1.0 is better )\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sim)\n",
    "rs = []\n",
    "print('Weighted_Similarity:')\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        score = sim.similar_to(peaks_extracted[i], peaks_extracted[j], method='weighted', rank=2)\n",
    "        rs += [(i, j, score)]\n",
    "\n",
    "rs = sorted(rs, key=lambda x: x[2], reverse=True) # sort all results in decending of similarity scores\n",
    "algo_score = 0\n",
    "for k, v in enumerate(rs):\n",
    "    i, j, s = v\n",
    "    tag = '*' if files[i][:4]==files[j][:4] else '' # tag the same strain id\n",
    "    if tag=='*':\n",
    "        print(f'#{k+1:2d}{tag}:\\t{files[i]} & {files[j]}\\t{s:.4f}{tag}')\n",
    "        algo_score += k;      \n",
    "\n",
    "p = n/2      # pairs of files         \n",
    "print(f'algo_score={2*algo_score/float(p*(p+1)):.2f} (closer to 1.0 is better )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c632d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
