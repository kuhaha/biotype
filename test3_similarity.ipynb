{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308fb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import similarity as sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d92aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L100_0_G7_1.txt\n",
      "L100_0_G8_1.txt\n",
      "L101_0_A1_1.txt\n",
      "L101_0_A2_1.txt\n",
      "L102_0_A3_1.txt\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR_PATH = \"datasets/export\"\n",
    "META_DIR_PATH = \"doc\"\n",
    "PEAK_DIR_PATH = \"extracted_peaks\"\n",
    "files =  os.listdir(DATA_DIR_PATH)\n",
    "files = [fn  for fn in files if fn.endswith(\".txt\")]\n",
    "files = sorted(files)\n",
    "for file in files[:5] :\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45719cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>菌株名</th>\n",
       "      <th>血清型</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MALDITOFMS\n",
       "Listeria serial No.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L001</th>\n",
       "      <td>LM1</td>\n",
       "      <td>1/2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L002</th>\n",
       "      <td>LM3</td>\n",
       "      <td>1/2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L003</th>\n",
       "      <td>LM4</td>\n",
       "      <td>4b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L004</th>\n",
       "      <td>LM7</td>\n",
       "      <td>1/2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L005</th>\n",
       "      <td>LM8</td>\n",
       "      <td>1/2a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 菌株名   血清型\n",
       "MALDITOFMS\\nListeria serial No.           \n",
       "L001                             LM1  1/2a\n",
       "L002                             LM3  1/2a\n",
       "L003                             LM4    4b\n",
       "L004                             LM7  1/2a\n",
       "L005                             LM8  1/2a"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.read_csv(META_DIR_PATH + '/meta.csv', index_col=0,encoding='utf-8')\n",
    "meta[['菌株名','血清型']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd58c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "serotype = meta['血清型'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e21216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L100_0_G7_1 (1/2b) peaks: 208\n",
      "L100_0_G8_1 (1/2b) peaks: 226\n",
      "L101_0_A1_1 (1/2a) peaks: 226\n",
      "L101_0_A2_1 (1/2a) peaks: 195\n",
      "L102_0_A3_1 (1/2b) peaks: 208\n",
      "L102_0_A4_1 (1/2b) peaks: 200\n",
      "L103_0_A5_1 (1/2b) peaks: 192\n",
      "L103_0_A6_1 (1/2b) peaks: 188\n",
      "L104_0_A7_1 (1/2a) peaks: 193\n",
      "L104_0_A8_1 (1/2a) peaks: 221\n",
      "L105_0_A10_1 (1/2b) peaks: 186\n",
      "L105_0_A9_1 (1/2b) peaks: 204\n",
      "L106_0_A11_1 (1/2a) peaks: 208\n",
      "L106_0_A12_1 (1/2a) peaks: 196\n",
      "L107_0_B1_1 (1/2b) peaks: 190\n",
      "L107_0_B2_1 (1/2b) peaks: 195\n",
      "L108_0_B3_1 (UT) peaks: 213\n",
      "L108_0_B4_1 (UT) peaks: 186\n",
      "L109_0_B5_1 (UT) peaks: 214\n",
      "L109_0_B6_1 (UT) peaks: 205\n",
      "L10_0_G10_1 (NA) peaks: 189\n",
      "L10_0_G9_1 (NA) peaks: 197\n",
      "L110_0_B7_1 (UT) peaks: 190\n",
      "L110_0_B8_1 (UT) peaks: 203\n",
      "L111_0_B10_1 (UT) peaks: 240\n",
      "L111_0_B9_1 (UT) peaks: 214\n",
      "L112_0_B11_1 (1/2a) peaks: 226\n",
      "L112_0_B12_1 (1/2a) peaks: 223\n",
      "L113_0_C1_1 (UT) peaks: 202\n",
      "L113_0_C2_1 (UT) peaks: 210\n",
      "L114_0_C3_1 (UT) peaks: 289\n",
      "L114_0_C4_1 (UT) peaks: 265\n",
      "L115_0_C5_1 (3a) peaks: 207\n",
      "L115_0_C6_1 (3a) peaks: 195\n",
      "L116_0_C7_1 (nan) peaks: 201\n",
      "L116_0_C8_1 (nan) peaks: 201\n",
      "L117_0_C10_1 (1/2a) peaks: 233\n",
      "L117_0_C9_1 (1/2a) peaks: 206\n",
      "L118_0_C11_1 (1/2a) peaks: 189\n",
      "L118_0_C12_1 (1/2a) peaks: 215\n",
      "L119_0_D1_1 (1/2a) peaks: 202\n",
      "L119_0_D2_1 (1/2a) peaks: 198\n",
      "L11_0_G11_1 (NA) peaks: 203\n",
      "L11_0_G12_1 (NA) peaks: 225\n",
      "L120_0_D3_1 (1/2a) peaks: 191\n",
      "L120_0_D4_1 (1/2a) peaks: 197\n",
      "L121_0_D5_1 (1/2b) peaks: 193\n",
      "L121_0_D6_1 (1/2b) peaks: 180\n",
      "L122_0_D7_1 (1/2b) peaks: 211\n",
      "L122_0_D8_1 (1/2b) peaks: 200\n",
      "L123_0_D10_1 (1/2a) peaks: 194\n",
      "L123_0_D9_1 (1/2a) peaks: 205\n",
      "L124_0_D11_1 (1/2a) peaks: 226\n",
      "L124_0_D12_1 (1/2a) peaks: 186\n",
      "L125_0_E1_1 (1/2a) peaks: 211\n",
      "L125_0_E2_1 (1/2a) peaks: 192\n",
      "L126_0_E3_1 (1/2b) peaks: 191\n",
      "L126_0_E4_1 (1/2b) peaks: 194\n",
      "L127_0_E5_1 (UT) peaks: 209\n",
      "L127_0_E6_1 (UT) peaks: 186\n",
      "L128_0_E7_1 (1/2a) peaks: 198\n",
      "L128_0_E8_1 (1/2a) peaks: 189\n",
      "L129_0_E10_1 (1/2b) peaks: 185\n",
      "L129_0_E9_1 (1/2b) peaks: 186\n",
      "L12_0_H1_1 (NA) peaks: 188\n",
      "L12_0_H2_1 (NA) peaks: 182\n",
      "L130_0_E11_1 (1/2a) peaks: 232\n",
      "L130_0_E12_1 (1/2a) peaks: 195\n",
      "L131_0_F1_1 (1/2a) peaks: 201\n",
      "L131_0_F2_1 (1/2a) peaks: 218\n",
      "L132_0_F3_1 (4b) peaks: 206\n",
      "L132_0_F4_1 (4b) peaks: 187\n",
      "L133_0_F5_1 (4b) peaks: 201\n",
      "L133_0_F6_1 (4b) peaks: 191\n",
      "L134_0_F7_1 (4b) peaks: 240\n",
      "L134_0_F8_1 (4b) peaks: 214\n",
      "L135_0_F10_1 (4b) peaks: 218\n",
      "L135_0_F9_1 (4b) peaks: 205\n",
      "L136_0_F11_1 (4b) peaks: 230\n",
      "L136_0_F12_1 (4b) peaks: 212\n",
      "L137_0_G1_1 (4b) peaks: 217\n",
      "L137_0_G2_1 (4b) peaks: 194\n",
      "L138_0_G3_1 (1/2c) peaks: 194\n",
      "L138_0_G4_1 (1/2c) peaks: 196\n",
      "L139_0_G5_1 (1/2c) peaks: 201\n",
      "L139_0_G6_1 (1/2c) peaks: 200\n",
      "L13_0_H3_1 (NA) peaks: 193\n",
      "L13_0_H4_1 (NA) peaks: 196\n",
      "L140_0_G7_1 (1/2c) peaks: 218\n",
      "L140_0_G8_1 (1/2c) peaks: 203\n",
      "L14_0_H5_1 (NA) peaks: 216\n",
      "L14_0_H6_1 (NA) peaks: 218\n",
      "L185_0_A10_1 (NA) peaks: 199\n",
      "L185_0_A9_1 (NA) peaks: 197\n",
      "L186_0_A11_1 (NA) peaks: 208\n",
      "L186_0_A12_1 (NA) peaks: 220\n",
      "L187_0_B1_1 (NA) peaks: 205\n",
      "L187_0_B2_1 (NA) peaks: 220\n",
      "L1_0_F3_1 (NA) peaks: 204\n",
      "L1_0_F4_1 (NA) peaks: 182\n",
      "L291_0_A1_1 (NA) peaks: 221\n",
      "L291_0_A2_1 (NA) peaks: 200\n"
     ]
    }
   ],
   "source": [
    "peaks_extracted = []\n",
    "n = len(files)\n",
    "for i in range(n):\n",
    "    df = None\n",
    "    df = pd.read_table(f\"{DATA_DIR_PATH}/{files[i]}\",sep=\" \", header=None,names=['m/z', 'intensity']) \n",
    "    x, y = df['m/z'].to_numpy(), df['intensity'].to_numpy()\n",
    "\n",
    "    pickle_file = files[i][:-4] # trim '.txt'\n",
    "    with open(f'{PEAK_DIR_PATH}/{pickle_file}_peaks.pkl', 'rb') as peak_file:\n",
    "        peaks = pickle.load(peak_file)\n",
    "        st = serotype.get(files[i][:4],'NA')\n",
    "        print(f'{pickle_file} ({st}) peaks: {len(peaks)}')\n",
    "        peaks_extracted +=  [(x[peaks], y[peaks]) ] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a9bfee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "n = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard_Similarity:\n",
      "# 6*:\tL121_0_D5_1.txt[1/2b] & L121_0_D6_1.txt[1/2b]\t0.4802*\n",
      "# 7*:\tL129_0_E10_1.txt[1/2b] & L129_0_E9_1.txt[1/2b]\t0.4781*\n",
      "#12*:\tL128_0_E7_1.txt[1/2a] & L128_0_E8_1.txt[1/2a]\t0.4715*\n",
      "#16*:\tL12_0_H1_1.txt[NA] & L12_0_H2_1.txt[NA]\t0.4683*\n",
      "#21*:\tL116_0_C7_1.txt[nan] & L116_0_C8_1.txt[nan]\t0.4672*\n",
      "#23*:\tL138_0_G3_1.txt[1/2c] & L138_0_G4_1.txt[1/2c]\t0.4662*\n",
      "#43*:\tL126_0_E3_1.txt[1/2b] & L126_0_E4_1.txt[1/2b]\t0.4474*\n",
      "#45*:\tL122_0_D7_1.txt[1/2b] & L122_0_D8_1.txt[1/2b]\t0.4472*\n",
      "#49*:\tL123_0_D10_1.txt[1/2a] & L123_0_D9_1.txt[1/2a]\t0.4457*\n",
      "#51*:\tL119_0_D1_1.txt[1/2a] & L119_0_D2_1.txt[1/2a]\t0.4440*\n",
      "#60*:\tL139_0_G5_1.txt[1/2c] & L139_0_G6_1.txt[1/2c]\t0.4424*\n",
      "#65*:\tL13_0_H3_1.txt[NA] & L13_0_H4_1.txt[NA]\t0.4407*\n",
      "#69*:\tL110_0_B7_1.txt[UT] & L110_0_B8_1.txt[UT]\t0.4396*\n",
      "#80*:\tL107_0_B1_1.txt[1/2b] & L107_0_B2_1.txt[1/2b]\t0.4366*\n",
      "#89*:\tL132_0_F3_1.txt[4b] & L132_0_F4_1.txt[4b]\t0.4343*\n",
      "#108*:\tL101_0_A1_1.txt[1/2a] & L101_0_A2_1.txt[1/2a]\t0.4320*\n",
      "#110*:\tL102_0_A3_1.txt[1/2b] & L102_0_A4_1.txt[1/2b]\t0.4316*\n",
      "#129*:\tL105_0_A10_1.txt[1/2b] & L105_0_A9_1.txt[1/2b]\t0.4286*\n",
      "#162*:\tL135_0_F10_1.txt[4b] & L135_0_F9_1.txt[4b]\t0.4242*\n",
      "#189*:\tL120_0_D3_1.txt[1/2a] & L120_0_D4_1.txt[1/2a]\t0.4212*\n",
      "#193*:\tL115_0_C5_1.txt[3a] & L115_0_C6_1.txt[3a]\t0.4205*\n",
      "#205*:\tL185_0_A10_1.txt[NA] & L185_0_A9_1.txt[NA]\t0.4194*\n",
      "#213*:\tL103_0_A5_1.txt[1/2b] & L103_0_A6_1.txt[1/2b]\t0.4179*\n",
      "#245*:\tL133_0_F5_1.txt[4b] & L133_0_F6_1.txt[4b]\t0.4152*\n",
      "#348*:\tL113_0_C1_1.txt[UT] & L113_0_C2_1.txt[UT]\t0.4061*\n",
      "#538*:\tL118_0_C11_1.txt[1/2a] & L118_0_C12_1.txt[1/2a]\t0.3931*\n",
      "#580*:\tL130_0_E11_1.txt[1/2a] & L130_0_E12_1.txt[1/2a]\t0.3909*\n",
      "#593*:\tL108_0_B3_1.txt[UT] & L108_0_B4_1.txt[UT]\t0.3902*\n",
      "#711*:\tL1_0_F3_1.txt[NA] & L1_0_F4_1.txt[NA]\t0.3835*\n",
      "#880*:\tL125_0_E1_1.txt[1/2a] & L125_0_E2_1.txt[1/2a]\t0.3754*\n",
      "#907*:\tL106_0_A11_1.txt[1/2a] & L106_0_A12_1.txt[1/2a]\t0.3741*\n",
      "#925*:\tL10_0_G10_1.txt[NA] & L10_0_G9_1.txt[NA]\t0.3737*\n",
      "#929*:\tL124_0_D11_1.txt[1/2a] & L124_0_D12_1.txt[1/2a]\t0.3733*\n",
      "#1102*:\tL140_0_G7_1.txt[1/2c] & L140_0_G8_1.txt[1/2c]\t0.3669*\n",
      "#1249*:\tL104_0_A7_1.txt[1/2a] & L104_0_A8_1.txt[1/2a]\t0.3618*\n",
      "#1296*:\tL109_0_B5_1.txt[UT] & L109_0_B6_1.txt[UT]\t0.3604*\n",
      "#1297*:\tL131_0_F1_1.txt[1/2a] & L131_0_F2_1.txt[1/2a]\t0.3604*\n",
      "#1345*:\tL186_0_A11_1.txt[NA] & L186_0_A12_1.txt[NA]\t0.3587*\n",
      "#1413*:\tL112_0_B11_1.txt[1/2a] & L112_0_B12_1.txt[1/2a]\t0.3565*\n",
      "#1421*:\tL137_0_G1_1.txt[4b] & L137_0_G2_1.txt[4b]\t0.3564*\n",
      "#1561*:\tL14_0_H5_1.txt[NA] & L14_0_H6_1.txt[NA]\t0.3520*\n",
      "#1893*:\tL11_0_G11_1.txt[NA] & L11_0_G12_1.txt[NA]\t0.3417*\n",
      "#1968*:\tL127_0_E5_1.txt[UT] & L127_0_E6_1.txt[UT]\t0.3390*\n",
      "#2063*:\tL187_0_B1_1.txt[NA] & L187_0_B2_1.txt[NA]\t0.3365*\n",
      "#2255*:\tL111_0_B10_1.txt[UT] & L111_0_B9_1.txt[UT]\t0.3314*\n",
      "#2297*:\tL117_0_C10_1.txt[1/2a] & L117_0_C9_1.txt[1/2a]\t0.3303*\n",
      "#2405*:\tL134_0_F7_1.txt[4b] & L134_0_F8_1.txt[4b]\t0.3275*\n",
      "#2562*:\tL136_0_F11_1.txt[4b] & L136_0_F12_1.txt[4b]\t0.3234*\n",
      "#3131*:\tL100_0_G7_1.txt[1/2b] & L100_0_G8_1.txt[1/2b]\t0.3072*\n",
      "#3430*:\tL291_0_A1_1.txt[NA] & L291_0_A2_1.txt[NA]\t0.2994*\n",
      "#4834*:\tL114_0_C3_1.txt[UT] & L114_0_C4_1.txt[UT]\t0.2422*\n",
      "algo_score=34.75 (closer to 1.0 is better )\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sim)\n",
    "rs = []\n",
    "print('Jaccard_Similarity:')\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        score = sim.similar_to(peaks_extracted[i], peaks_extracted[j], method='jaccard', rank=2)\n",
    "        rs += [(i, j, score)]\n",
    "\n",
    "rs = sorted(rs, key=lambda x: x[2], reverse=True)\n",
    "algo_score=0\n",
    "for k, v in enumerate(rs):\n",
    "    i, j, s = v\n",
    "    tag = '*' if files[i][:4]==files[j][:4] else '' # tag the same strain id\n",
    "    st1 =  serotype.get(files[i][:4],'NA')\n",
    "    st2 =  serotype.get(files[j][:4],'NA')\n",
    "    if tag=='*':\n",
    "        algo_score += k\n",
    "        print(f'#{k+1:2d}{tag}:\\t{files[i]}[{st1}] & {files[j]}[{st2}]\\t{s:.4f}{tag}')\n",
    "\n",
    "p = n/2      # pairs of files   \n",
    "print(f'algo_score={2*algo_score/float(p*(p+1)):.2f} (closer to 1.0 is better )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ee4a05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank_Similarity:\n",
      "# 1*:\tL116_0_C7_1.txt[nan] & L116_0_C8_1.txt[nan]\t0.3686*\n",
      "# 4*:\tL119_0_D1_1.txt[1/2a] & L119_0_D2_1.txt[1/2a]\t0.3105*\n",
      "# 6*:\tL138_0_G3_1.txt[1/2c] & L138_0_G4_1.txt[1/2c]\t0.3045*\n",
      "#69*:\tL185_0_A10_1.txt[NA] & L185_0_A9_1.txt[NA]\t0.1935*\n",
      "#146*:\tL129_0_E10_1.txt[1/2b] & L129_0_E9_1.txt[1/2b]\t0.1554*\n",
      "#189*:\tL14_0_H5_1.txt[NA] & L14_0_H6_1.txt[NA]\t0.1402*\n",
      "#503*:\tL139_0_G5_1.txt[1/2c] & L139_0_G6_1.txt[1/2c]\t0.0863*\n",
      "#678*:\tL123_0_D10_1.txt[1/2a] & L123_0_D9_1.txt[1/2a]\t0.0688*\n",
      "#693*:\tL126_0_E3_1.txt[1/2b] & L126_0_E4_1.txt[1/2b]\t0.0677*\n",
      "#994*:\tL13_0_H3_1.txt[NA] & L13_0_H4_1.txt[NA]\t0.0444*\n",
      "#1069*:\tL112_0_B11_1.txt[1/2a] & L112_0_B12_1.txt[1/2a]\t0.0393*\n",
      "#1074*:\tL109_0_B5_1.txt[UT] & L109_0_B6_1.txt[UT]\t0.0390*\n",
      "#1217*:\tL102_0_A3_1.txt[1/2b] & L102_0_A4_1.txt[1/2b]\t0.0316*\n",
      "#1242*:\tL106_0_A11_1.txt[1/2a] & L106_0_A12_1.txt[1/2a]\t0.0306*\n",
      "#1261*:\tL107_0_B1_1.txt[1/2b] & L107_0_B2_1.txt[1/2b]\t0.0299*\n",
      "#1342*:\tL103_0_A5_1.txt[1/2b] & L103_0_A6_1.txt[1/2b]\t0.0261*\n",
      "#1428*:\tL135_0_F10_1.txt[4b] & L135_0_F9_1.txt[4b]\t0.0236*\n",
      "#1531*:\tL114_0_C3_1.txt[UT] & L114_0_C4_1.txt[UT]\t0.0202*\n",
      "#1606*:\tL136_0_F11_1.txt[4b] & L136_0_F12_1.txt[4b]\t0.0180*\n",
      "#1779*:\tL120_0_D3_1.txt[1/2a] & L120_0_D4_1.txt[1/2a]\t0.0147*\n",
      "#1838*:\tL125_0_E1_1.txt[1/2a] & L125_0_E2_1.txt[1/2a]\t0.0137*\n",
      "#1875*:\tL104_0_A7_1.txt[1/2a] & L104_0_A8_1.txt[1/2a]\t0.0132*\n",
      "#1938*:\tL187_0_B1_1.txt[NA] & L187_0_B2_1.txt[NA]\t0.0126*\n",
      "#1975*:\tL117_0_C10_1.txt[1/2a] & L117_0_C9_1.txt[1/2a]\t0.0121*\n",
      "#1978*:\tL100_0_G7_1.txt[1/2b] & L100_0_G8_1.txt[1/2b]\t0.0120*\n",
      "#2001*:\tL128_0_E7_1.txt[1/2a] & L128_0_E8_1.txt[1/2a]\t0.0114*\n",
      "#2028*:\tL110_0_B7_1.txt[UT] & L110_0_B8_1.txt[UT]\t0.0110*\n",
      "#2071*:\tL115_0_C5_1.txt[3a] & L115_0_C6_1.txt[3a]\t0.0106*\n",
      "#2129*:\tL127_0_E5_1.txt[UT] & L127_0_E6_1.txt[UT]\t0.0102*\n",
      "#2250*:\tL186_0_A11_1.txt[NA] & L186_0_A12_1.txt[NA]\t0.0095*\n",
      "#2392*:\tL105_0_A10_1.txt[1/2b] & L105_0_A9_1.txt[1/2b]\t0.0073*\n",
      "#2397*:\tL132_0_F3_1.txt[4b] & L132_0_F4_1.txt[4b]\t0.0073*\n",
      "#2421*:\tL10_0_G10_1.txt[NA] & L10_0_G9_1.txt[NA]\t0.0071*\n",
      "#2494*:\tL113_0_C1_1.txt[UT] & L113_0_C2_1.txt[UT]\t0.0068*\n",
      "#2563*:\tL137_0_G1_1.txt[4b] & L137_0_G2_1.txt[4b]\t0.0066*\n",
      "#2934*:\tL1_0_F3_1.txt[NA] & L1_0_F4_1.txt[NA]\t0.0036*\n",
      "#2972*:\tL122_0_D7_1.txt[1/2b] & L122_0_D8_1.txt[1/2b]\t0.0035*\n",
      "#3015*:\tL118_0_C11_1.txt[1/2a] & L118_0_C12_1.txt[1/2a]\t0.0034*\n",
      "#3052*:\tL101_0_A1_1.txt[1/2a] & L101_0_A2_1.txt[1/2a]\t0.0034*\n",
      "#3246*:\tL140_0_G7_1.txt[1/2c] & L140_0_G8_1.txt[1/2c]\t0.0032*\n",
      "#3468*:\tL291_0_A1_1.txt[NA] & L291_0_A2_1.txt[NA]\t0.0031*\n",
      "#3590*:\tL111_0_B10_1.txt[UT] & L111_0_B9_1.txt[UT]\t0.0029*\n",
      "#3601*:\tL134_0_F7_1.txt[4b] & L134_0_F8_1.txt[4b]\t0.0029*\n",
      "#4044*:\tL108_0_B3_1.txt[UT] & L108_0_B4_1.txt[UT]\t0.0000*\n",
      "#4649*:\tL11_0_G11_1.txt[NA] & L11_0_G12_1.txt[NA]\t0.0000*\n",
      "#4735*:\tL121_0_D5_1.txt[1/2b] & L121_0_D6_1.txt[1/2b]\t0.0000*\n",
      "#4795*:\tL124_0_D11_1.txt[1/2a] & L124_0_D12_1.txt[1/2a]\t0.0000*\n",
      "#4953*:\tL12_0_H1_1.txt[NA] & L12_0_H2_1.txt[NA]\t0.0000*\n",
      "#4978*:\tL130_0_E11_1.txt[1/2a] & L130_0_E12_1.txt[1/2a]\t0.0000*\n",
      "#5008*:\tL131_0_F1_1.txt[1/2a] & L131_0_F2_1.txt[1/2a]\t0.0000*\n",
      "#5051*:\tL133_0_F5_1.txt[4b] & L133_0_F6_1.txt[4b]\t0.0000*\n",
      "algo_score=85.39 (closer to 1.0 is better )\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sim)\n",
    "rs = []\n",
    "print('Rank_Similarity:')\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        score = sim.similar_to(peaks_extracted[i], peaks_extracted[j], method='rank', rank=2)\n",
    "        rs += [(i, j, score)]\n",
    "\n",
    "rs = sorted(rs, key=lambda x: x[2], reverse=True)\n",
    "algo_score=0\n",
    "for k, v in enumerate(rs):\n",
    "    i, j, s = v\n",
    "    tag = '*' if files[i][:4]==files[j][:4] else '' # tag the same strain id\n",
    "    st1 =  serotype.get(files[i][:4],'NA')\n",
    "    st2 =  serotype.get(files[j][:4],'NA')\n",
    "    if tag=='*':\n",
    "        algo_score += k\n",
    "        print(f'#{k+1:2d}{tag}:\\t{files[i]}[{st1}] & {files[j]}[{st2}]\\t{s:.4f}{tag}')\n",
    "\n",
    "p = n/2      # pairs of files   \n",
    "print(f'algo_score={2*algo_score/float(p*(p+1)):.2f} (closer to 1.0 is better )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a4cd337",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted_Similarity:\n",
      "# 2*:\tL116_0_C7_1.txt & L116_0_C8_1.txt\t0.0469*\n",
      "# 3*:\tL119_0_D1_1.txt & L119_0_D2_1.txt\t0.0460*\n",
      "# 5*:\tL138_0_G3_1.txt & L138_0_G4_1.txt\t0.0400*\n",
      "#18*:\tL185_0_A10_1.txt & L185_0_A9_1.txt\t0.0329*\n",
      "#39*:\tL129_0_E10_1.txt & L129_0_E9_1.txt\t0.0296*\n",
      "#193*:\tL139_0_G5_1.txt & L139_0_G6_1.txt\t0.0190*\n",
      "#306*:\tL14_0_H5_1.txt & L14_0_H6_1.txt\t0.0159*\n",
      "#384*:\tL126_0_E3_1.txt & L126_0_E4_1.txt\t0.0144*\n",
      "#758*:\tL123_0_D10_1.txt & L123_0_D9_1.txt\t0.0102*\n",
      "#1019*:\tL107_0_B1_1.txt & L107_0_B2_1.txt\t0.0086*\n",
      "#1157*:\tL109_0_B5_1.txt & L109_0_B6_1.txt\t0.0078*\n",
      "#1255*:\tL13_0_H3_1.txt & L13_0_H4_1.txt\t0.0073*\n",
      "#1295*:\tL120_0_D3_1.txt & L120_0_D4_1.txt\t0.0071*\n",
      "#1308*:\tL110_0_B7_1.txt & L110_0_B8_1.txt\t0.0071*\n",
      "#1484*:\tL102_0_A3_1.txt & L102_0_A4_1.txt\t0.0064*\n",
      "#1505*:\tL104_0_A7_1.txt & L104_0_A8_1.txt\t0.0063*\n",
      "#1676*:\tL135_0_F10_1.txt & L135_0_F9_1.txt\t0.0056*\n",
      "#1694*:\tL106_0_A11_1.txt & L106_0_A12_1.txt\t0.0056*\n",
      "#1783*:\tL125_0_E1_1.txt & L125_0_E2_1.txt\t0.0053*\n",
      "#1884*:\tL132_0_F3_1.txt & L132_0_F4_1.txt\t0.0050*\n",
      "#2003*:\tL112_0_B11_1.txt & L112_0_B12_1.txt\t0.0046*\n",
      "#2050*:\tL103_0_A5_1.txt & L103_0_A6_1.txt\t0.0044*\n",
      "#2057*:\tL105_0_A10_1.txt & L105_0_A9_1.txt\t0.0044*\n",
      "#2195*:\tL136_0_F11_1.txt & L136_0_F12_1.txt\t0.0039*\n",
      "#2279*:\tL128_0_E7_1.txt & L128_0_E8_1.txt\t0.0037*\n",
      "#2319*:\tL10_0_G10_1.txt & L10_0_G9_1.txt\t0.0036*\n",
      "#2329*:\tL117_0_C10_1.txt & L117_0_C9_1.txt\t0.0036*\n",
      "#2414*:\tL291_0_A1_1.txt & L291_0_A2_1.txt\t0.0034*\n",
      "#2421*:\tL137_0_G1_1.txt & L137_0_G2_1.txt\t0.0034*\n",
      "#2455*:\tL113_0_C1_1.txt & L113_0_C2_1.txt\t0.0033*\n",
      "#2538*:\tL115_0_C5_1.txt & L115_0_C6_1.txt\t0.0031*\n",
      "#2562*:\tL114_0_C3_1.txt & L114_0_C4_1.txt\t0.0031*\n",
      "#2613*:\tL187_0_B1_1.txt & L187_0_B2_1.txt\t0.0029*\n",
      "#2787*:\tL122_0_D7_1.txt & L122_0_D8_1.txt\t0.0024*\n",
      "#2807*:\tL118_0_C11_1.txt & L118_0_C12_1.txt\t0.0024*\n",
      "#2945*:\tL127_0_E5_1.txt & L127_0_E6_1.txt\t0.0020*\n",
      "#2967*:\tL1_0_F3_1.txt & L1_0_F4_1.txt\t0.0020*\n",
      "#3100*:\tL186_0_A11_1.txt & L186_0_A12_1.txt\t0.0017*\n",
      "#3121*:\tL100_0_G7_1.txt & L100_0_G8_1.txt\t0.0016*\n",
      "#3193*:\tL134_0_F7_1.txt & L134_0_F8_1.txt\t0.0015*\n",
      "#3266*:\tL140_0_G7_1.txt & L140_0_G8_1.txt\t0.0013*\n",
      "#3315*:\tL111_0_B10_1.txt & L111_0_B9_1.txt\t0.0012*\n",
      "#3344*:\tL101_0_A1_1.txt & L101_0_A2_1.txt\t0.0011*\n",
      "#4044*:\tL108_0_B3_1.txt & L108_0_B4_1.txt\t0.0000*\n",
      "#4649*:\tL11_0_G11_1.txt & L11_0_G12_1.txt\t0.0000*\n",
      "#4735*:\tL121_0_D5_1.txt & L121_0_D6_1.txt\t0.0000*\n",
      "#4795*:\tL124_0_D11_1.txt & L124_0_D12_1.txt\t0.0000*\n",
      "#4953*:\tL12_0_H1_1.txt & L12_0_H2_1.txt\t0.0000*\n",
      "#4978*:\tL130_0_E11_1.txt & L130_0_E12_1.txt\t0.0000*\n",
      "#5008*:\tL131_0_F1_1.txt & L131_0_F2_1.txt\t0.0000*\n",
      "#5051*:\tL133_0_F5_1.txt & L133_0_F6_1.txt\t0.0000*\n",
      "algo_score=88.24 (closer to 1.0 is better )\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sim)\n",
    "rs = []\n",
    "print('Weighted_Similarity:')\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1,n):\n",
    "        score = sim.similar_to(peaks_extracted[i], peaks_extracted[j], method='weighted', rank=2)\n",
    "        rs += [(i, j, score)]\n",
    "\n",
    "rs = sorted(rs, key=lambda x: x[2], reverse=True) # sort all results in decending of similarity scores\n",
    "algo_score = 0\n",
    "for k, v in enumerate(rs):\n",
    "    i, j, s = v\n",
    "    tag = '*' if files[i][:4]==files[j][:4] else '' # tag the same strain id\n",
    "    if tag=='*':\n",
    "        print(f'#{k+1:2d}{tag}:\\t{files[i]} & {files[j]}\\t{s:.4f}{tag}')\n",
    "        algo_score += k;      \n",
    "\n",
    "p = n/2      # pairs of files         \n",
    "print(f'algo_score={2*algo_score/float(p*(p+1)):.2f} (closer to 1.0 is better )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c632d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
